---
title: RLVR
weight: 10
---


# RLVR


# 可验证奖励强化学习 (RLVR) 技术白皮书

## 1.0 引言：重塑大模型能力边界的新范式

大型语言模型（LLM）的“后训练”（Post-training）阶段，是将其从一个原始的、未经雕琢的预训练模型，塑造为对用户真正有用、安全且遵循指令的智能工具的关键环节。在这一战略性阶段，各种技术范式不断涌现，旨在精准地调优模型的行为与能力。其中，**可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR)** 作为一种新兴且强大的训练范式，正日益受到业界的广泛关注。

本白皮书旨在全面解析 RLVR 的核心原理、技术演进脉络，及其在提升 LLM 数学推理、代码生成等核心能力方面的具体应用与未来潜力。我们希望通过深度的分析与洞察，为人工智能领域的专业人士、研究人员及开发者提供一个清晰、完整的技术蓝图。

为了充分理解 RLVR 的创新之处，我们有必要首先回顾其技术根源。接下来，我们将从强化学习在语言模型领域的历史演进开始，追溯从“从人类反馈中学习的强化学习”（RLHF）到 RLVR 的范式变革之路。

## 2.0 从 RLHF 到 RLVR：一种范式的演进

技术的进步往往遵循着清晰的演进脉络，新的范式总是在继承与革新中诞生。可验证奖励强化学习 (RLVR) 并非凭空出现，它是对早期强化学习范式，特别是“从人类反馈中学习的强化学习”（Reinforcement Learning from Human Feedback, RLHF）的直接回应与发展。本章节将追溯这一发展路径，以揭示 RLVR 诞生的必然性及其旨在解决的核心问题。

### 2.1 奠基石：从人类反馈中学习的强化学习 (RLHF)

RLHF 的核心思想是利用人类的偏好数据来训练一个“奖励模型”（Reward Model），这个模型能够评估模型生成内容的优劣，并以此为信号，通过强化学习算法来指导和优化大型语言模型的策略。简而言之，它教会模型“什么是更好的回答”。

以 `**InstructGPT**` 和 `**ChatGPT**` 的巨大成功为例，RLHF 发挥了至关重要的催化剂作用。它不仅开启了流畅、自然的聊天交互模式时代，更重要的是，它为后续更复杂的强化学习应用奠定了坚实的基础架构，并点燃了整个行业对 RL 与语言模型结合的浓厚兴趣。

从本质上看，RLHF 是一种将复杂、模糊甚至难以言传的人类价值观与偏好注入到模型中的有效方式。然而，这一过程的复杂性也为其带来了与生俱来的挑战。

### 2.2 RLHF 的挑战与局限性

RLHF 在实践中面临着多重挑战，这些挑战共同推动了业界对更高效、更直接的训练范式的探索：

- **数据采集的复杂性**: 收集高质量、大规模、一致性强的人类偏好数据，是一项成本高昂且耗时巨大的工程。
- **过程的“混乱”与跨学科性**: RLHF 的实施远非一个纯粹的算法问题。它涉及心理学、社会学以及难以量化的基础性问题，整个流程“混乱且跨学科”（messy and interdisciplinary），难以实现标准化和规模化复制。
- **主观性与不一致性**: 完全依赖人类标注员的主观偏好，不可避免地会引入个人或群体的偏见，为奖励模型的稳定性和可靠性带来了不确定性。

与其将这些挑战视为有待“解决”的问题，不如将其看作是 tackling 基础性、主观性对齐任务时固有的复杂性。RLHF 试图解决的是“何为良好价值观”这类根本性难题，这决定了其在学术上具有更长远的研究前景。

### 2.3 RLVR 的兴起：更直接、更高效的路径

RLVR 的出现，并非作为 RLHF 的替代品，而是一种务实的、以行业为中心的**优先级转变**。它巧妙地绕开了 RLHF 试图解决的“混乱”的基础性问题，转而专注于那些能够带来可衡量性能提升的客观任务。其核心变革在于，**用一个客观的、可被程序自动验证的“计分函数”（Scoring Function）或“验证器”（Verifier），取代了基于人类偏好的、主观的奖励模型。**

这一转变具有重大的战略意义。RLVR 的目标极为明确和简单——“让模型的分数变得更高”（make the scores better）。这一目标与行业的核心需求高度契合，因为它能够直接、可量化地提升模型在特定任务上的性能，尤其是在那些具有明确“正确答案”（ground truth）的领域，如数学问题求解、代码生成与调试、以及事实性知识遵循等。

下表清晰地对比了 RLHF 与 RLVR 之间的核心区别：

| 特性         | **强化学习来自人类反馈 (RLHF)**    | **可验证奖励强化学习 (RLVR)**                  |
| ------------ | ---------------------------------- | ---------------------------------------------- |
| **奖励来源** | 基于人类偏好数据的奖励模型 (主观)  | 基于任务结果的确定性计分函数 (客观)            |
| **核心目标** | 对齐人类价值观、风格和偏好         | 提升在可验证任务上的性能分数                   |
| **应用领域** | 风格塑造、安全性、通用对话能力     | 数学推理、代码生成、事实遵循                   |
| **主要挑战** | 数据收集成本高、过程复杂、主观性强 | 仅适用于有明确验证标准（“ground truth”）的领域 |

总而言之，RLVR 不仅仅是一次技术上的简化，更代表了后训练理念的一次重要转变——从解决模糊的基础性对齐问题，转向追求可度量的任务性能。这一转变，为我们深入剖析 RLVR 的技术核心奠定了基础。

## 3.0 RLVR 的核心原理与技术框架

在理解了 RLVR 的演进背景后，本章节将深入其技术内部，详细剖析它在现代 LLM 后训练流程中的具体定位、关键组成部分以及核心运作机制。我们的目标是为读者构建一个清晰、准确的 RLVR 技术蓝图。

### 3.1 RLVR 在现代后训练流程中的定位

在现代 LLM 的后训练实践中（如 Tulu 3 模型所展示的），通常包含一个多阶段、目标各有侧重的流程。RLVR 在其中扮演着一个独特且关键的角色。我们可以将这个流程分解为三个递进的阶段：

1. **监督微调 (Supervised Fine-Tuning, SFT)**: 这是后训练的基石。SFT 的主要作用是**“教授格式和响应形态”**，通过高质量的“指令-回答”对，让模型学会如何遵循指令、以特定的结构（如聊天格式）生成内容。它为模型打下了行为基础。
2. **偏好微调 (Preference Tuning, 如 DPO)**: 这是 RLHF 理念的现代化、泛化实现。它通过对比数据（例如，“回答A”优于“回答B”）来教会模型**“更像什么，更不像什么”**。这一阶段主要用于优化模型的风格、语调，并能间接提升推理路径的合理性，使其更符合人类偏好。
3. **强化微调 (Reinforcement Fine-Tuning, 即 RLVR)**: 这是三个阶段中**“最开放”（most open-ended）**的环节。我们订阅“能力激发观”（elicitation view）来理解其作用。这一观点至关重要，因为它将 RLVR 的角色重新定义为：**它并非在教授模型新知识，而是创建一个强大的激励信号，以放大那些在预训练后已存在但概率较低的潜在能力。** 通过奖励那些成功解决任务的输出，RLVR 极大地提升了模型生成正确答案的概率，从而在数学、代码等硬核能力上实现突破。

### 3.2 RLVR 机制的关键组成部分

若将经典的强化学习框架（智能体-环境）与 LLM 的应用进行类比，会发现一个显著的变化。在 LLM 的 RLVR 实践中，“环境”（Environment）的概念被极大地简化了。模型（智能体）实际上是与一系列独立的提示词（状态）和一个奖励函数进行交互，而非在一个连续的、有状态记忆的环境中探索。

RLVR 的核心技术替换体现在以下方面： **用“验证器”（Verifier）取代“奖励模型”（Reward Model）**。

与需要大量数据训练的奖励模型不同，验证器通常是一个简单的、确定性的函数。其工作原理非常直接：

- 如果模型生成的答案是**正确**的，则给予一个正向奖励（例如 `+1`）。
- 如果答案是**错误**的，则不给予奖励（即奖励为 `0`）。

这种非黑即白的奖励机制，使得训练信号非常清晰，直指最终的任务目标。

### 3.3 验证方法与应用领域

RLVR 的强大之处在于其验证方法的多样性与可扩展性，能够应用于任何可以定义明确验证规则的领域。以下是一些典型的验证方法及其应用场景：

- **数学问题验证** 通过 `math_verify` 等开源库或内部开发的工具，解析模型生成的最终答案，并检查其是否与标准答案相符。这对于提升模型在 `GSM8K` 等数学基准测试上的表现至关重要。
- **代码执行验证** 通过在隔离的**代码沙箱（Code Sandboxes）**中运行模型生成的代码，并检查其是否能成功通过预设的单元测试（Unit Tests）。这是提升模型编程和软件工程能力的关键手段。
- **事实性知识验证** 使用一个能力更强的“语言模型作为裁判”（LM as a Judge）来判断模型回答的事实准确性。虽然这是一种相对“较软”的验证形式，但它将可验证性的边界从绝对正确扩展到了事实一致性领域。
- **指令遵循验证** 对于特定的格式要求，可以设计简单的检查函数。例如，对于“请用全小写字母回答 IPv6 的作用”这一指令，验证器只需检查返回的文本中是否不含大写字母即可。

在处理更复杂的多领域问题时，单一的 `+1/0` 奖励可能不足。此时，可以引入**“奖励塑造”（Reward Shaping）**的概念，设计更精细的奖励机制来权衡答案的不同方面（如步骤的正确性、最终结果的准确性等），从而引导模型产生更理想的行为。

总而言之，RLVR 的核心框架简洁而强大，其可扩展性使其能够持续赋能 LLM 在各类可精确评估的任务上取得进步。接下来，我们将探讨支撑这一框架的具体算法。

## 4.0 关键算法与实施考量

一个成功的理论框架，离不开高效算法的支撑以及对实践中各种细微差别的深刻理解。本章节将聚焦于当前支撑 RLVR 的核心算法，特别是 GRPO，并深入探讨在实际部署过程中必须面对的挑战与权衡。

### 4.1 算法演进：从 PPO 到 GRPO

在语言模型的强化学习应用中，一个显著的行业趋势是，越来越多的方法正在减少对传统“价值函数”（Value Functions）的依赖。在此背景下，由 DeepSeek 提出并成功应用的 **GRPO (Group Relative Policy Optimization)** 算法脱颖而出，它作为经典 PPO 算法的变体，展现出优异的性能。GRPO 的关键特征在于：

- **优势函数 (Advantage) 的计算**: 这是 GRPO 的核心创新。一个生成补全（completion）的“优势”，是根据其奖励与**同一提示词下其他所有补全的平均奖励**进行比较得出的。这带来了一个深刻的后果：如果在处理单个提示的批次（batch）中，所有生成的答案都正确（奖励均为+1）或都错误（奖励均为0），那么每个答案的优势都将为零。**这意味着模型将不会从该样本中进行任何学习更新。** 这一机制是 GRPO 效率的关键所在：它自动将学习过程聚焦于模型产生对错混杂的“边界”案例上——这正是模型最需要学习的地方。
- **KL 惩罚项的位置**: 在 GRPO 中，用于防止模型策略偏离过远的 KL 散度惩罚项被直接置于损失函数中。这与许多传统 RLHF 实践中将其作为奖励的一部分有所不同，是一个影响训练动态的重要实现细节。

### 4.2 算法细节的权衡：长度偏见与归一化

GRPO 的成功也催生了一系列变体算法，如 `DAPO` 和 `Dr-GRPO`，它们试图在细微之处进行优化。然而，虽然这些算法变体在学术上很有趣，但一个来自实践的关键教训是：**实现质量和数据分布的性质，其影响力往往远超算法上的细微调整。** 与其将新论文视为保证有效的银弹，不如将其看作是学习 RL 机制的宝贵课程。

这些算法调整试图解决的一个核心问题是**“长度偏见”（Length Bias）**。这种偏见源于一个事实：一条长而正确的推理链是一系列 token 概率相乘的产物，这使得其整体序列概率天生较低。在按 token 计算的策略梯度更新中，这条低概率但高价值的序列可能比较短、价值较低的序列获得不成比例的小更新权重。不同的归一化策略正是为了抵消这种效应，确保有价值的长篇推理得到应有的奖励。

### 4.3 训练过程中的挑战：方差与可复现性

与 SFT 和 DPO 相比，RLVR 训练过程表现出显著更高的方差。这一挑战给研发和部署带来了实际的困难：

- **对随机种子的敏感性**: 使用不同的随机种子启动训练，最终得到的模型性能可能会有天壤之别。
- **收益的不可预测性**: 有时 RLVR 阶段能带来巨大的性能提升（例如，在基准测试上+2个百分点），而有时其收益却微乎其微，其中的原因尚不完全清楚。
- **行业实践**: 这种高方差迫使工业界采取一种务实的、近乎“暴力”的方法：并行启动多个具有不同随机种子的 RL 训练任务。通过密切监控早期的学习曲线，表现不佳的运行会被主动“扼杀”，以便将计算资源重新导向少数展现出良好早期轨迹的任务。
- **资源投入的权衡**: 实验表明，“让模型烹饪更久”（let it cook longer），即延长训练时间，有时确实能获得更好的模型。但这与宝贵的 GPU 资源的投资回报率（ROI）之间存在着一种难以预测的权衡。

尽管存在这些挑战，RLVR 凭借其巨大的潜力，已经展现出不可替代的应用价值。下一章，我们将具体分析其应用案例与所取得的丰硕成果。

## 5.0 应用案例、实验成果与最佳实践

理论和算法的价值最终需要通过实际应用来检验。本章节将展示 RLVR 在不同场景下的具体应用，分析其带来的显著性能提升，并总结从大量实验中提炼出的关键洞见与最佳实践。

### 5.1 核心应用：提升数学与编程等关键能力

RLVR 最直接、最主流的应用场景，是在标准的后训练流程（SFT 和 DPO）之后，增加一个专门的 RLVR 阶段。这一阶段的目标非常明确：**集中火力提升模型在数学和编程等可被精确评估的硬核能力上的分数。**

在实践中，研究人员还探索出一种被称为**“迭代式 RLVR”（Iterative RLVR）**的有效技巧。其操作方式是，在一个已经过少量 RLVR 训练的模型检查点（checkpoint）上，重新开始新一轮的 RLVR 训练。实验表明，这种迭代优化的方法有时能够进一步压榨出模型的性能潜力，实现分数的持续攀升。

一个令人鼓舞的观察是，RLVR 的训练曲线通常表现得相当稳定，其形态与标准的强化学习实践非常相似，这大大降低了其在工程上实施和调试的门槛。

### 5.2 范式拓展：直接在基础模型上进行大规模 RL

这种方法与 5.1 节中寻求的增量改进形成鲜明对比。其目标不仅仅是提升一个已具能力的模型的分数，而是从根本上**“激发”**出基础模型中如“思维链”（Chain-of-Thought）等复杂的推理行为，使其经历一次更为剧烈的策略转变，之后再通过偏好微调等手段进行风格或安全性的对齐。

这种更具雄心的应用范式是：**直接在预训练完成的基础模型（Base Model）上进行大规模的 RLVR 训练**，DeepSeek 的 Q* 模型便是这一范式的典型代表。

### 5.3 性能分析：显著的增益与宝贵的特性

综合评估 RLVR 带来的性能影响，其成果是显著但多变的。我们可以将其核心价值总结为以下两个方面：

- 性能提升 (Performance Enhancement):
  - 在数学和代码等关键基准测试上，RLVR 能够带来显著的分数提升，例如相比 DPO 后的模型，有时能高出 2 个百分点。
  - 通过算法优化（如从 PPO 切换到 GRPO）和迭代式训练等技巧，可以获得更大的性能飞跃。
- 能力保持 (Capability Retention):
  - 一个关键且令人惊喜的发现是，RLVR 在专门提升目标能力（如数学）的同时，通常**不会导致模型在其他通用能力上出现明显下降**。例如，衡量通用对话能力的 `AlpacaEval` 分数能够保持稳定。
  - 这证明了现代 LLM 具有巨大的学习容量，可以在不产生“灾难性遗忘”的情况下，高效地学习和整合新技能。

实践证明，RLVR 是一种高效且相对安全的模型能力增强技术。它的成功应用不仅提升了现有模型的性能上限，也为我们指明了通往更强大人工智能的未来方向。

## 6.0 未来方向与展望

作为一种新兴的、以结果为导向的后训练方法，可验证奖励强化学习 (RLVR) 已经取得了令人瞩目的成功。尽管对于强化学习的纯粹主义者而言，RLVR 的核心原则——奖励智能体正确的产出——听起来可能有些基础，但其真正的颠覆性在于它在现代 LLM 后训练栈中的易用性、可扩展性以及惊人的有效性。当前的研究和应用很可能仅仅是冰山一角。本章节将探讨 RLVR 未来可能演进的几个重要方向，及其可能对整个大型语言模型发展格局产生的深远影响。

### 6.1 扩展可验证性的边界

RLVR 的一个核心发展方向，是将其应用范围从数学、代码等“硬”可验证领域，逐步扩展到事实性问答、遵循复杂指令等“半”可验证领域。我们可以构想一个**“可验证性谱系”（Spectrum of Verifiability）**：在谱系的一端，是具有绝对正确答案的任务，它们是 RLVR 的主场；在另一端，是完全主观的任务，这依然是偏好微调的优势领域。未来的后训练流程将不再是单一技术的线性组合，而是会根据任务的可验证程度，灵活地结合多种技术，以达到最优效果。

### 6.2 与推理时计算的协同进化

当前，RLVR 是实现**“推理时计算扩展”（Inference-time Scaling）**最主流的训练手段，即通过训练让模型生成更长、更复杂的推理链来提升准确率。然而，这种关系可能会在未来发生演变。随着并行计算等其他推理时优化技术的成熟，或许我们不再需要通过 RL 无限度地增加模型的单次生成长度。未来的趋势可能是在**训练成本**（通过 RL 教会模型长推理链）与**推理效率**（通过更高效的推理时技术获得答案）之间寻求一个更优的平衡点。

### 6.3 终极愿景：RL 是否会成为 LLM 训练的核心？

最后，我们不妨提出一个大胆的设想：**RLVR 或更广义的强化学习训练，未来是否可能成为 LLM 开发的焦点？**

一个引人深思的案例是，DeepSeek V3 论文中后训练仅占总计算资源的 0.2%，而其后续 Q* 模型的 RL 训练则耗时数周。这表明，后训练，特别是 RL 阶段的计算占比正在以前所未有的速度急剧增加。我们可以描绘一个未来的图景：LLM 的开发过程，可能演变为在一个包含多种验证器和 RL 环境的复杂系统中，通过大规模、多领域的强化学习训练，来持续迭代和提升一个核心智能体的通用能力。如果这一天到来，它将从根本上改变当前以预训练为主导的计算资源分配格局。

RLVR 的出现，不仅仅是一次技术工具的进步。它更可能预示着我们关于如何构建和发展大型语言模型这一核心理念的深刻变革。这条通往更强通用智能的道路，值得整个 AI 社区持续关注、探索和投入。



# PPT

[RLVR](https://candied-skunk-1ca.notion.site/RLVR-2b4bfe21108480009ae7c3bfc7ebb8b2?source=copy_link)



# 参考

Notebook 视频转报告

bili 
