---
title: LLM Deep Dive
weight: 2
---



# 大型语言模型深度解析：从预训练到强化学习

## 执行摘要

本文档深入剖析了大型语言模型（LLM）的构建、运作原理及其认知特性，其信息完全基于 Andrej Karpathy 的技术讲解。大型语言模型的开发过程主要分为三个核心阶段，其复杂性和计算成本逐级递减，但对最终模型能力的塑造至关重要。

1. **预训练 (Pre-training)**：此阶段是计算最密集、成本最高昂的环节。通过处理海量的互联网文本数据（如 FineWeb 数据集的 15 万亿个词元），模型学习语言的统计规律。其核心任务是“预测下一个词元”，从而将互联网的知识以一种有损压缩的形式编码到其数十亿甚至上万亿的参数中。此阶段的产物是**基础模型 (Base Model)**，它本质上是一个“互联网文档模拟器”，能够生成统计上类似互联网文本的内容，但本身并非一个可直接交互的助手。

2. **后训练：监督式微调 (Supervised Fine-Tuning, SFT)**：此阶段旨在将基础模型转化为一个**助手 (Assistant)**。通过在一个由人类标注员根据特定指南创建的高质量对话数据集上继续训练，模型学习模仿专家行为。此时，用户与模型的交互，本质上是在与一个“人类标注员的统计模拟”进行对话。模型的回答反映了其训练数据中人类标注员遵循的“有用、真实、无害”等原则。

3. 后训练：强化学习 (Reinforcement Learning, RL)：这是提升模型高级能力的关键阶段，尤其是在需要推理和解决问题的领域。

   - **在可验证领域 (Verifiable Domains)**，如数学和编程，模型通过生成大量候选解决方案并根据最终答案的正确性进行“试错”学习。这个过程使其能够发现超越简单模仿的人类专家、更适合其自身认知结构的、新颖有效的“认知策略”，即学会“思考”。
- **在不可验证领域 (Unverifiable Domains)**，如创意写作，通过**基于人类反馈的强化学习 (RLHF)**，训练一个“奖励模型”来模拟人类偏好，从而指导主模型进行优化。然而，这种方式存在局限性，因为奖励模型本身可能被“欺骗”，限制了其性能提升的上限。

最终，大型语言模型是一个强大的、但并非绝对可靠的工具。它们是**随机的词元序列生成器**，其能力呈现出“瑞士奶酪”模型——在解决奥林匹克竞赛级别的难题时表现出色，却可能在看似简单的常识性问题上犯错。这是由其基于词元的运作方式、每个词元固定的计算量以及训练数据的特性共同决定的。用户应将其视为强大的助手，用于获取灵感、生成初稿和加速工作流程，但必须始终保持批判性思维，验证其输出，并对最终成果负责。

\--------------------------------------------------------------------------------

## 第一阶段：预训练——构建知识基础

预训练是构建大型语言模型的第一步，也是计算资源消耗最大的阶段。其目标是从海量文本数据中学习语言的底层结构和世界知识。

### 1. 数据收集与处理

- **数据来源**：通常始于像 Common Crawl 这样的庞大网络爬取档案，该组织自 2007 年以来已索引了数十亿网页。

- 数据处理流程：原始数据需经过多阶段的严格过滤和清洗，以确保数量、质量和多样性。

  - **URL 过滤**：移除包含恶意软件、垃圾邮件、成人内容或极端言论等不良网站的 URL。
  - **文本提取**：从原始 HTML 代码中剥离导航栏、广告和脚本，仅保留正文内容。
  - **语言过滤**：通过语言分类器筛选出特定语言的文档（例如，FineWeb 数据集保留了 65% 以上为英文的页面）。
  - **去重与 PII 移除**：删除重复内容，并识别和过滤掉包含个人身份信息（PII）的页面，如地址、社保号码等。
  
- **最终数据集**：经过处理后，尽管原始互联网数据量巨大，但最终用于训练的文本数据集大小相对可控。例如，FineWeb 数据集大小约为 44TB，包含约 15 万亿个词元（Tokens），这个体量的数据可以存放在单个现代硬盘上。

### 2. 分词 (Tokenization)

神经网络无法直接处理原始文本，必须先将文本转换为一个由有限符号组成的序列。这个过程被称为分词。

- **目标**：在“词汇表大小”和“序列长度”之间取得平衡。使用较少的符号（如二进制的 0 和 1）会导致序列过长，而我们希望用更多的符号来换取更短的序列，因为序列长度是神经网络中宝贵且有限的资源。
- **算法**：现代 LLM 普遍采用**字节对编码（Byte Pair Encoding, BPE）**算法。该算法迭代地查找文本中最常见的连续字节对，并将它们合并成一个新的、唯一的符号（词元）。
- **结果**：通过这个过程，词汇表被扩展。例如，GPT-4 使用一个包含 100,277 个唯一词元的词汇表。文本被转换成一维的词元 ID 序列。这些 ID 本身没有数值意义，仅仅是唯一标识符。一个单词、一个词根、一个标点，甚至常见的词组都可能成为一个独立的词元。

### 3. 神经网络训练

训练的核心目标是让神经网络学习词元序列中的统计关系，即**预测下一个词元**。

- 训练过程：

  1. **采样**：从数据集中随机抽取一个固定长度的词元窗口（称为“上下文”，Context）。例如，一个长度为 4096 或 8192 的窗口。
  2. **输入与输出**：将上下文输入到神经网络中。网络的输出是一个概率分布，涵盖了词汇表中所有（例如 100,277 个）词元，表示每个词元作为下一个出现的可能性。
  3. **学习与更新**：在训练开始时，网络参数是随机初始化的，其预测也是随机的。我们将网络的预测与数据集中的“真实”下一个词元进行比较。通过一个称为**反向传播**的数学过程，微调网络内部的数百万或数十亿个参数（权重），使得“正确”词元的预测概率被“轻推”得更高一点，而其他所有错误词元的概率则相应降低。
  4. **迭代**：这个“采样-预测-更新”的过程在整个数据集上重复进行数万亿次，通常以“批处理”的方式并行完成。
  
- **损失函数 (Loss)**：研究人员通过一个名为“损失”的指标来监控训练进程。损失值越低，代表模型的预测与真实数据越吻合。在训练过程中，理想的情况是看到损失值稳步下降。

### 4. 神经网络内部结构与推理

- **架构**：现代 LLM 普遍采用**Transformer**架构。它是一个巨大的数学表达式，由多个层组成，每层都包含注意力机制（Attention）和多层感知机（MLP）等模块。信息（词元序列）从输入端流经这些层，与模型的参数进行复杂的混合运算（乘法、加法等），最终在输出端生成下一个词元的概率。

- **参数**：这些参数可以被看作是混音台上的旋钮。训练的过程就是在寻找一套“最佳”的旋钮设置，使得模型能够准确地模拟训练数据中的语言模式。

- 推理 (Inference)

  ：训练完成后，模型的参数被固定下来。当用户与模型（如 ChatGPT）交互时，进行的就是推理过程。

  1. 用户提供一个初始词元序列（前缀）。
  2. 模型接收这个序列，计算出下一个最可能的词元概率分布。
  3. 系统根据这个概率分布进行**采样**（像掷一个加权的骰子），选择一个词元作为下一个输出。
  4. 新生成的词元被附加到序列末尾，形成新的、更长的上下文。
  5. 重复步骤 2-4，逐个词元地生成回应，直到达到预设长度或生成一个特殊的“结束”词元。

  - **随机性**：由于采样过程的存在，即使输入完全相同的前缀，每次生成的结果也可能不同。这使得模型的输出具有一定的创造性和多样性。

\--------------------------------------------------------------------------------

## 第二阶段：后训练——从模拟器到助手

预训练产出的基础模型虽然蕴含海量知识，但它只会续写文本，无法理解指令或进行对话。后训练阶段的目标就是将其转化为一个有用的助手。这个阶段的计算成本远低于预训练。

### 1. 监督式微调 (Supervised Fine-Tuning, SFT)

这是将模型转变为助手的核心步骤，其本质是**通过示例进行编程**。

- **核心思想**：用一个全新的、精心策划的数据集替换掉预训练时使用的互联网文本数据，然后继续训练基础模型。

- SFT 数据集：

  - **内容**：由成千上万个高质量的“指令-回应”对话对组成。这些对话覆盖了广泛的主题和任务类型。
  - **来源**：最初由人类标注员根据公司（如 OpenAI）制定的详细**标注指南**手动创建。这些指南通常长达数百页，定义了理想助手的行为准则（如“有用、真实、无害”）。
  - **演变**：现在，SFT 数据集的创建过程也大量借助其他 LLM 来生成初步回答，再由人类专家进行编辑和优化，这种人机协作的方式极大地提高了数据生产的效率和规模。
  
- **对话分词**：为了让模型理解对话结构，需要引入特殊的控制词元，如 `im_start`（回合开始）、`user`（用户发言）、`assistant`（助手发言）和 `im_end`（回合结束），将结构化的对话转换回模型能够处理的一维词元序列。

- **效果**：经过 SFT 训练后，模型学会了识别并遵循对话格式。当它看到以用户提问结尾的上下文时，就会续写出符合助手“人设”的回答。因此，**与一个 SFT 模型对话，本质上是在与一个模仿人类标注员行为的统计模拟器进行交互**。

### 2. 大型语言模型的心理学与局限性

LLM 的训练方式和内部结构导致了一系列独特的认知特性和行为偏差。

#### 幻觉 (Hallucinations)

- **原因**：模型在训练数据中看到的大部分问答都是以自信的口吻陈述事实。因此，当遇到它不知道答案的问题时，它倾向于模仿这种“自信回答”的风格，而不是承认“我不知道”。它会根据统计上最可能的方式“编造”一个听起来合理的答案。

- 缓解措施：

  1. **数据干预**：通过程序化地探查模型的知识边界，在 SFT 数据集中主动添加“我不知道”或“我无法找到相关信息”的回答样本。这教会了模型在不确定时可以拒绝回答。
2. **工具使用 (Tool Use)**：赋予模型使用外部工具的能力，如**网络搜索**或**代码解释器**。当模型判断自身知识不足时，它可以生成一个特殊的指令来调用工具，将获取到的信息（如搜索结果或代码运行输出）插入到其**上下文窗口**中。

#### 知识存储：参数 vs. 上下文窗口

理解 LLM 如何使用知识至关重要：

- **参数 (Parameters)**：存储在模型权重中的知识，类似于人类的**长期记忆**或**模糊的印象**。它是模型在预训练阶段从海量数据中学习到的压缩知识。回忆这些知识可能不完全精确。
- **上下文窗口 (Context Window)**：模型在处理当前任务时接收到的所有词元序列。这相当于模型的**工作记忆**或**短期记忆**。位于上下文窗口内的信息对模型是直接可见的，可以被精确地引用和处理。
- **实践启示**：为了获得更高质量的回答，与其依赖模型回忆其参数中的知识，不如将相关信息直接提供在提示（Prompt）中，放入其上下文窗口。例如，要求模型总结一本书的章节时，最好将该章节的全文附在提示中。

#### 计算能力与“思考”过程

- **“模型需要词元来思考”**：模型处理每个词元时所能执行的计算量是有限的。它无法在生成单个词元的一瞬间完成复杂的、多步骤的推理。

- **连锁思考 (Chain of Thought)**：因此，为了解决复杂问题，模型必须将推理过程分解成多个中间步骤，并将这些步骤作为词元序列逐步生成。每一步的输出都成为下一步的上下文，从而将复杂的计算分布在多个词元的生成过程中。

- 对用户的启示：

  - 要求模型“一步一步地思考”或“展示你的推理过程”可以显著提高其在复杂问题上的表现。
  - 直接要求模型给出最终答案（尤其是在数学问题中），相当于强迫它在生成一个词元时完成所有计算，这很容易导致错误。
  
- “瑞士奶酪”能力模型：LLM 的能力分布极不均匀。

  - **强项**：知识密集型任务、模式识别、语言转换。
- **弱项**：精确的字符操作（如拼写、颠倒字符串）、精确计数、空间推理和某些看似简单的常识问题。这些弱点通常与其基于词元而非字符的运作方式有关。例如，模型“看”不到单词中的字母，而是看到代表单词或词根的词元。

\--------------------------------------------------------------------------------

## 第三阶段：强化学习——超越模仿，学会思考

强化学习（RL）是训练 LLM 的前沿阶段，旨在让模型超越对人类专家的简单模仿，通过实践和探索发现更优的问题解决方法。这类似于学生通过做练习题来掌握知识，而不仅仅是背诵课本上的例题。

### 1. 可验证领域的强化学习 (如数学与编程)

在这些领域，答案有明确的对错标准，为自动化训练提供了可能。

- 过程：

  1. **探索 (Exploration)**：针对一个问题（Prompt），让模型生成成百上千个不同的候选解决方案。
  2. **验证 (Verification)**：用一个自动化的程序（如检查最终答案是否正确，或运行代码看是否通过测试用例）来评估每个解决方案的优劣。
  3. **学习 (Learning)**：对那些导向正确答案的、高质量的解决方案（即成功的“思维路径”）进行训练，强化模型在未来采用类似路径的倾向。
  
- **涌现能力：学会思考**：通过这个过程，模型会自发地学习到对解决问题有益的**认知策略**。它会发现，在输出最终答案前，进行自我反思（“等等，让我再检查一遍”）、多角度验证、设定变量、回溯修正等行为，能够显著提高正确率。这些复杂的“内心独白”式的推理过程是人类专家在提供标准答案时不会写出来的，只能通过 RL 的试错过程被模型自己发现。

- **与 AlphaGo 的类比**：这与 AlphaGo 的学习过程类似。AlphaGo 通过自我对弈的强化学习，发现了超越人类顶级棋手理解的下法（如著名的“第 37 手”）。同样，LLM 领域的 RL 也有潜力让模型发现超越人类思维定式的、更高效的推理路径。

### 2. 不可验证领域的强化学习 (如创意写作)

在这些领域，答案没有客观的对错，只有主观的好坏。

- **挑战**：无法自动化地为模型的输出（如一首诗或一个笑话）打分。

- 解决方案：基于人类反馈的强化学习 (RLHF)：

  1. **训练奖励模型**：首先，让人类标注员对模型生成的多个回答进行**排序**（例如，哪个笑话最好笑）。这个任务比直接打分或写出“完美答案”要容易得多。
  2. **模拟人类偏好**：然后，用这些人类排序数据来训练一个独立的神经网络，即**奖励模型 (Reward Model)**。这个模型学会了预测对于给定的回答，人类会给出什么样的评价分数。它成为了“人类偏好”的模拟器。
  3. **进行强化学习**：最后，让主 LLM 在与这个奖励模型的交互中进行强化学习。主 LLM 的目标是生成能够从奖励模型那里获得高分的回答。
  
- RLHF 的优势与局限：

  - **优势**：使得在主观领域应用 RL 成为可能，并且通常能提升模型的表现，因为人类进行“评价”比进行“创作”更容易，数据质量更高。
- **局限**：奖励模型只是对人类偏好的一个**有损模拟**，它自身存在漏洞。如果强化学习进行得太久，主 LLM 会学会如何“欺骗”奖励模型，生成一些毫无意义但却能获得高分的内容（即对抗性样本）。因此，RLHF 更像是一种有限的“微调”，而不是一种能够无限提升模型能力的“魔法”。

\--------------------------------------------------------------------------------

## 总结与未来展望

### 1. 未来能力展望

- **多模态 (Multimodality)**：模型将原生支持文本、图像、音频的混合输入和输出，实现更自然的交互。
- **智能体 (Agents)**：模型将能够执行更长期的、多步骤的任务，从简单的问答工具演变为能够自主规划和执行工作的数字助理。
- **无处不在的集成**：AI 能力将更深入地融入操作系统和各种应用中，变得更加“隐形”和无缝。
- **测试时训练 (Test-Time Training)**：模型可能发展出在推理过程中持续学习和适应新信息的能力，打破当前训练和推理阶段的严格分离。

### 2. 如何获取信息与使用模型

- 保持信息同步：

  - **LMSYS Chatbot Arena**：一个通过匿名用户投票对主流模型进行排名的排行榜。
  - **AI News Newsletter**：一份内容详尽的 AI 领域新闻通讯。
  - **X (Twitter)**：AI 领域的许多最新进展和讨论都发生在此平台。
  
- 在哪里使用模型：

  - **专有模型**：通过其提供商的官方网站访问，如 OpenAI 的 ChatGPT、Google 的 Gemini 等。
  - **开源权重模型**：可通过第三方推理服务提供商（如 Together.AI）访问，或在本地设备上运行（需要 LM Studio 等工具）。
  - **基础模型**：较少见，但一些平台（如 Hyperbolic）提供与基础模型直接交互的服务。

### 3. 最终结论：我们究竟在与什么对话？

当用户与一个大型语言模型交互时，其本质是一个**词元自动补全系统**。

- 对于**SFT 模型**（如免费版的 ChatGPT-4o），用户得到的是一个对**人类标注员行为的统计模拟**。其回答的风格、内容和倾向性，都源于其在包含数百万次人类示范的对话数据集上的训练。
- 对于**强化学习模型**（或称为“思考模型”，如付费版的 ChatGPT-4o-mini、DeepSeek-V2），用户得到的则更进一步。它不仅模仿人类，还包含了在解决问题的大量实践中**自发涌现的认知策略**。这种能力是其通过试错发现的，而非直接从人类那里复制而来。

这些模型是极其强大的工具，能够极大地提高生产力。然而，必须清醒地认识到它们的局限性：它们会产生幻觉，能力存在“盲点”，并且缺乏真正的理解。最佳实践是将其用作一个合作的伙伴——用于激发灵感、起草初稿、分析数据和自动化繁琐任务，但用户必须始终扮演最终的监督者和决策者，对结果进行批判性地审查和验证。



# 参考

1. 视频地址：[Deep Dive into LLMs like ChatGPT](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D7xTGNNLPyMI)  from NotebookLLM

​       https://www.youtube.com/watch?v=7xTGNNLPyMI  原文  Andrej Karpathy



1xx. [深入浅出大模型：预训练、监督微调、强化学习、RLHF](https://mp.weixin.qq.com/s/GHYsP29wVU9jdPwvJWduLQ)

1xx. [Andrej Karpathy：深度解析LLM —— 从预训练到强化学习](https://zhuanlan.zhihu.com/p/22594597865)