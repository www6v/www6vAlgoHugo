---
title: LLM架构演进
weight: 10
---


# 效率与能力的平衡：LLM架构演进与关键设计权衡深度解析

### **1. 执行摘要 (Executive Summary)**

现代大语言模型（Large Language Model, LLM）的架构演进，本质上是各大研究机构在 **计算效率** 与 **模型能力** 的双重约束下所做出的一系列战略性“赌注”。本报告深度剖析了这一趋势，并指出两条主导性策略正浮出水面，以应对规模化扩展的困境：一是通过**稀疏专家混合 (Mixture-of-Experts, MoE)** 架构实现计算高效的容量扩展；二是通过**多头潜在注意力 (Multi-Head Latent Attention, MLA)** 等创新机制，实现内存高效的上下文处理。这些设计选择不仅揭示了技术路线的分歧，更反映了不同组织在性能、训练稳定性与推理成本之间的核心权衡。

\--------------------------------------------------------------------------------

### **2. 核心架构对比矩阵**

本节通过一个结构化的对比矩阵，系统性地梳理和评估视频中讨论的各个LLM架构的关键特征、创新点及其带来的具体影响，为读者提供一个清晰的横向比较视图。下表提炼了各个模型在架构设计上的核心取舍，揭示了行业在效率与能力权衡中的多样化探索。

| 模型架构                    | 核心创新与特点                                             | 设计权衡与影响                                               | 视频论据引用                                                 |
| --------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **DeepSeek v3**             | 多头潜在注意力 (MLA)；拥有共享专家的MoE架构                | **MLA**：通过引入一个压缩状态来存储键值缓存 `(KV Cache)`，以少量额外计算为代价，实现了内存占用的急剧下降，显著降低了长上下文推理的硬件门槛。<br>**MoE**：以更高的训练复杂性换取巨大的模型容量和显著降低的推理激活参数量。 | > DeepSeek的MLA与MHA相比，KV缓存参数量从86万降至3.4万，实现了超过20倍的压缩 `[视频未提供时间戳]`。<br>> 模型总参数为6710亿，但推理时激活参数仅为370亿 `[视频未提供时间戳]`。 |
| **Olmo 2**                  | 残差连接内部的后置归一化 (Post-Norm) 布局；QK Norm         | 追求极致的训练稳定性。相比传统的前置归一化 (Pre-Norm)，该设计旨在减少训练过程中的梯度尖峰，实现更平滑、更可控的优化过程，降低训练失败风险。 | > 视频中的图表显示，与Pre-Norm相比，Olmo 2采用的Post-Norm布局使得梯度L2范数曲线更平滑，显著减少了训练不稳定的尖峰 `[视频未提供时间戳]`。 |
| **Gemma 3**                 | 滑动窗口注意力；“Pre-Norm + Post-Norm”双重归一化 + QK Norm | 这种混合局部/全局注意力的策略是一种务实的妥协，使模型能处理长上下文，同时避免了纯全局注意力带来的内存成本二次方增长，从而降低了部署的硬件要求。 | > Gemma 3采用5:1的局部/全局注意力层比例，在长上下文场景下，其内存成本增长远低于纯全局注意力模型 `[视频未提供时间戳]`。 |
| **Mistral Small 3.1**       | 浅层宽架构                                                 | 相对于Gemma 3的深层窄架构，Mistral选择了更少但更宽的Transformer块。这种设计优先考虑 **推理吞吐量**，因为序贯计算的层数更少，从而降低了交互式应用的延迟。 | > 视频作者推测，其更少的Transformer块（40 vs Gemma 3的62）可能是其推理速度更快的原因，这是一种在推理延迟和服务成本上的权衡 `[视频未提供时间戳]`。 |
| **Llama 4 (Maverick)**      | 独特的MoE配置，激活极少数量的大型专家                      | 采用一种独特的MoE配置，激活专家数极少（一个共享专家，一个常规专家），但专家本身规模巨大。此设计路径与DeepSeek的“多数小型专家”形成鲜明对比，对路由效率和专家特化程度提出了不同挑战。 | > Llama 4 (400B) 每次仅激活两个专家（一个共享，一个常规），激活参数为700亿，但其每个专家的中间层维度（8100）远大于DeepSeek（2000）`[视频未提供时间戳]`。 |
| **Qwen 3 (Dense & Sparse)** | **稠密版**：深层窄架构<br>**稀疏版**：无共享专家           | **深层窄架构**：与Llama 3相比，以更慢的推理速度换取了显著更低的内存占用，优化了部署成本。<br>**无共享专家**：简化了MoE的训练设计，但可能牺牲了由共享专家学习通用知识所带来的整体学习效率。 | > 作者的实现数据显示，Qwen 3（更深）内存占用约1.5GB，推理速度24 tokens/sec；而Llama 3（更宽）内存占用约3GB，速度为42 tokens/sec `[视频未提供时间戳]`。 |
| **GPT-OSS**                 | MoE架构；恒定宽度的前馈网络 (FFN)                          | 采用非传统的恒定宽度FFN（中间层维度等于输入维度），而非主流的“先扩张后压缩”的倒沙漏形态。这是一种探索参数效率的新尝试，但可能限制了模型在FFN层中的信息处理能力。 | > 传统的FFN中间层维度通常是输入的4倍，而GPT-OSS保持不变，与Qwen 3的“沙漏”形态（收缩）和主流的“倒沙漏”形态（扩张）均不同 `[视频未提供时间戳]`。 |
| **Kimmy 2**                 | 大量细粒度专家；首层为稠密块                               | 将“多数小型专家”的趋势推向极致，旨在实现更精细化的知识分工。首层采用稠密块是一种工程上的稳定策略，用于防止专家在训练初期过早“崩溃”。 | > 拥有1万亿总参数，但激活参数仅320亿，比规模更小的DeepSeek v3（370亿）更高效。其专家数量远超其他模型 `[视频未提供时间戳]`。 |

该矩阵揭示了三种相互竞争的设计哲学：1) **通过新颖注意力机制追求超高效率**，以DeepSeek的MLA为代表；2) **通过创新的归一化策略追求极致的训练稳定性**，由Olmo 2和Gemma 3引领；3) **在吞吐量与内存占用之间进行优化**，体现在Qwen、Llama和Mistral的深/浅层架构选择中。这些相互竞争的哲学体现为三个核心的架构争议点，我们将在下文中进行深入剖析。

\--------------------------------------------------------------------------------

### **3. 关键架构争议点分析**

理解当前LLM架构设计的核心争议点，对于评估不同模型的技术路线、预测未来发展趋势以及在实际应用中做出明智的技术选型至关重要。这些争议本质上是在 **模型性能、推理成本** 和 **训练稳定性** 这个“铁三角”之间寻求最优解的不同路径。

#### **3.1 稠密模型 (Dense) vs. 稀疏模型 (Sparse/MoE)**

**定义与对比** **稠密模型 (Dense Model)** 指在每次前向传播计算中，模型的所有参数都会被激活和使用，是传统Transformer架构的标准形态 `[视频未提供时间戳]`。相比之下，**稀疏模型 (Sparse Model)** 以**专家混合 (Mixture-of-Experts, MoE)** 为代表，其拥有海量的总参数，但在处理每个输入时，仅通过一个**路由器 (router)** 激活一小部分参数（即“专家”），从而在保持巨大模型容量的同时实现计算上的稀疏性 `[视频未提供时间戳]`。

**稀疏模型的优势** MoE架构的核心价值在于 **“以更低的推理成本获得更大的模型容量”**。通过将知识分布式地存储在大量专家中，模型可以获得极高的知识容量，但在推理时只需调用一小部分专家，从而保持了较低的计算负载。

例如，DeepSeek v3拥有6710亿总参数，但在推理时仅激活370亿参数；Llama 4拥有4000亿总参数，激活700亿参数。这使得巨大模型在实际部署中成为可能 `[视频未提供时间戳]`。 此外，部分MoE架构引入了**共享专家 (shared expert)** 的概念。该专家每次都被激活，负责学习所有任务都需要的通用知识（如语法、标点符号），从而避免了每个独立专家重复学习这些基础知识，释放其容量以专注于更特定的领域，提升了整体学习效率 `[视频未提供时间戳]`。

**设计演进与挑战** MoE架构的设计趋势正从早期的 **“少数大型专家”** 演变为 **“多数小型化、细粒度的专家”**，这种设计被认为能实现更专业的知识分工 `[视频未提供时间戳]`。然而，稀疏模型也带来了显著的工程挑战：其训练过程更为复杂，需要精心设计路由器、引入辅助损失项来平衡专家负载，并时刻面临“专家崩溃”等训练不稳定的风险 `[视频未提供时间戳]`。

**稠密模型的战略价值** 尽管稀疏模型优势显著，稠密架构仍保有其战略价值，它提供了简化的训练流程和更低的操作复杂性。**Qwen 3同时发布稠密和稀疏版本的策略**便是一个力证。对于那些将上市时间和训练稳定性置于首要位置的项目而言，避免MoE路由和负载均衡等复杂问题，选择一个更易于训练和优化的稠密架构，仍然是一个极具吸引力的工程选择 `[视频未提供时间戳]`。

#### **3.2 归一化层布局策略：Pre-Norm vs. Post-Norm**

**背景阐述** 归一化层，如 `Layer Normalization` 或更现代的 `RMS Norm`，在Transformer架构中对稳定训练至关重要。其布局策略主要有两种：**Post-Norm (后置归一化)**，原始Transformer的选择，位于残差连接之后；以及 **Pre-Norm (前置归一化)**，自GPT-2以来的主流选择，位于多头注意力和前馈网络之前，以提升深度网络的训练稳定性 `[视频未提供时间戳]`。

**新范式分析** 近期模型对这一传统布局发起了挑战，探索更优的稳定性与性能平衡点。

- **Olmo 2的“内部Post-Norm”**：Olmo 2创新地将Post-Norm布局移至 **残差连接的内部**。
- **Gemma 3的“双重归一化”**：Gemma 3采取了更为极致的策略，同时使用了 **“Pre-Norm + Post-Norm”**，并在注意力计算中额外加入了 **QK Norm**（对Query和Key向量进行归一化）。这种多重保障的设计哲学，反映了其对 **追求极致训练稳定性** 的不懈努力，即使会带来微小的计算开销 `[视频未提供时间戳]`。

**总结权衡** 归一化层布局的争议核心在于 **训练稳定性与最终模型性能** 之间的权衡。Olmo 2和Gemma 3等模型的创新表明，业界并未满足于现有的Pre-Norm范式，而是正在积极探索能够同时实现稳定训练和高性能的更优解决方案。

#### **3.3 架构形态之争：深层窄架构 vs. 浅层宽架构**

**识别模式** 视频中反复出现一个关键的设计权衡：模型的 **深度**（Transformer块的数量）与 **宽度**（前馈网络中间层维度、注意力头数）之间的选择。这一选择直接影响了模型的多个关键特性。

**案例对比分析**

- **Qwen 3 vs. Llama 3**：Qwen 3采用了更深、更窄的设计，而Llama 3则更浅、更宽。
- **Mistral 3.1 vs. Gemma 3**：这对组合也呈现了类似的模式。Mistral架构更浅、更宽，而Gemma 3则更深、更窄。这再次体现了模型设计者在 **推理延迟和服务成本** 与 **模型参数效率** 之间的不同取舍 `[视频未提供时间戳]`。

**结论提炼** 深/窄与浅/宽的选择没有绝对的优劣，而是服务于特定业务目标的工程决策。**浅层宽架构优先考虑低延迟的用户体验，因此更受交互式应用的青睐；而深层窄架构则优化了参数效率和内存成本，使其更适合批处理任务或在资源受限的硬件上进行部署。** 这一权衡直接决定了模型的 **推理速度、内存占用和参数效率**。





# 参考
+ NotebookLLM 视频 转 报告
+ [开源大语言模型架构全景图：11种主流LLM深度对比](https://www.bilibili.com/video/BV1xax4ziEZd)
+ https://www.youtube.com/watch?v=rNlULI-zGcw
