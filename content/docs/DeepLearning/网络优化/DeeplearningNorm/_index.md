---
title: è§„èŒƒåŒ– Norm
date: 2024-01-08 19:40:39
weight: 3
tags:
  - deepLearning
categories: 
  - AIGC
  - deepLearning   
---

<p></p>
<!-- more -->



# Norm ä½œç”¨[1]

dnn çš„æ ‡å‡†ç»„ä»¶ï¼Œç¨³å®šå’ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹

# Batch Norm[1]

- reduce cross **batch size**
- **mini-batch dimension**

ä¸€èˆ¬ç”¨äºå›¾åƒï¼Œä¸æ¶‰åŠåˆ°paddingçš„é—®é¢˜ï¼›

# Layer Norm[1]

- reduce cross **hidden dim**
- reduce across the **feature dimension**.

ä¸€èˆ¬ç”¨äºåºåˆ—ï¼Œä¸€ä¸ª batch size å†…å­˜åœ¨ paddingï¼›

- RMSNorm: å¯¹ LN çš„ä¸€ç§å˜ä½“ï¼Œllama

---

<aside> ğŸ’¡

- https://spaces.ac.cn/archives/9009
- Pre LN: `llama`
- Post LN: `attention is all you need`

llamaåœ¨å·¥ç¨‹ä¸Šä½¿ç”¨Pre LN

</aside>

---

1. [[pytorch\] BNã€LNã€RMSNorm åŠ pre LN vs. post LN å¯¹æ¯”ï¼Œæ ‡å‡†åŒ–](https://www.bilibili.com/video/BV13q49eaERj/)  v ***

â€‹	[normalization.ipynb](https://github.com/chunhuizhang/llm_aigc/blob/main/tutorials/nn_basics/tricks_norms/normalization.ipynb)

â€‹	[[pytorch\] BNã€LNã€RMSNorm åŠ pre LN vs. post LN å¯¹æ¯”ï¼Œæ ‡å‡†åŒ– ](https://www.notion.so/pytorch-BN-LN-RMSNorm-pre-LN-vs-post-LN-177bfe2110848088830cfea3d5a33d3e?pvs=21)

1xx. [Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations](https://afterhoursresearch.hashnode.dev/batch-normalization-layer-normalization-and-root-mean-square-layer-normalization-a-comprehensive-guide-with-python-implementations)

todo

- [7.5 é€å±‚è§„èŒƒåŒ–](https://www.notion.so/7-5-174bfe2110848045bc6cff467363d471?pvs=21) ç™¾åº¦é‚±  æœ‰ä»£ç 

  https://aistudio.baidu.com/education/lessonvideo/3048901
